#!/usr/bin/env python3
"""
æ·±åœ³ä¿éšœæˆ¿æ•°æ®æ”¶é›†å™¨ - æ··åˆæ–¹æ¡ˆå®ç°
ç»“åˆå¤šç§æ•°æ®æºï¼Œç¡®ä¿æ•°æ®å‡†ç¡®å¯é 
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import time
import os
from urllib.parse import urljoin

class HousingDataFetcher:
    """ä¿éšœæˆ¿æ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        self.config_dir = os.path.expanduser("~/.sz-housing")
        self.data_file = os.path.join(self.config_dir, "notices.json")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        # å®˜æ–¹æ•°æ®æºé…ç½®
        self.sources = {
            "sz_zjj": {
                "name": "æ·±åœ³å¸‚ä½æˆ¿å’Œå»ºè®¾å±€",
                "base_url": "https://zjj.sz.gov.cn",
                "notice_url": "https://zjj.sz.gov.cn/xxgk/tzgg/",
                "housing_url": "https://zjj.sz.gov.cn/ztfw/zfbz/"
            },
            "futian": {
                "name": "ç¦ç”°åŒºä½å»ºå±€",
                "base_url": "https://www.szft.gov.cn",
                "notice_url": "https://www.szft.gov.cn/bmxx/qjsj/tzgg/"
            },
            "longhua": {
                "name": "é¾™ååŒºä½å»ºå±€",
                "base_url": "https://www.szlhq.gov.cn",
                "notice_url": "https://www.szlhq.gov.cn/lhq/zdfwgb/zfztgb/zxgg38/"
            },
            "guangming": {
                "name": "å…‰æ˜åŒºä½å»ºå±€",
                "base_url": "https://www.szgm.gov.cn",
                "notice_url": "https://www.szgm.gov.cn/gmjsj/zcfg/"
            }
        }

    def fetch_page(self, url, max_retries=3):
        """è·å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                print(f"  è·å– {url} å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return None

    def parse_notice_list(self, html, base_url):
        """è§£æå…¬å‘Šåˆ—è¡¨é¡µ"""
        notices = []
        soup = BeautifulSoup(html, 'html.parser')

        # å¤šç§é€‰æ‹©å™¨æ¨¡å¼ï¼ˆé€‚åº”ä¸åŒç½‘ç«™ç»“æ„ï¼‰
        selectors = [
            'ul li a[href*="content/post"]',  # æ·±åœ³å¸‚æ”¿åºœé€šç”¨æ¨¡å¼
            'a[href*="/tzgg/content/"]',      # é€šçŸ¥å…¬å‘Šé“¾æ¥
            '.notice-list a',                  # é€šçŸ¥åˆ—è¡¨ç±»
            '.article-list a',                 # æ–‡ç« åˆ—è¡¨ç±»
            'ul.list-txt li a',                # æ–‡æœ¬åˆ—è¡¨
            '.txt-list li a'                   # å¦ä¸€ç§æ–‡æœ¬åˆ—è¡¨
        ]

        for selector in selectors:
            links = soup.select(selector)
            if links:
                print(f"  ä½¿ç”¨é€‰æ‹©å™¨: {selector}, æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                break
        else:
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ‰€æœ‰åŒ…å«"é…å”®"ã€"é…ç§Ÿ"çš„é“¾æ¥
            print("  ä½¿ç”¨å…³é”®è¯æœç´¢...")
            all_links = soup.find_all('a', href=True)
            links = [a for a in all_links if any(kw in a.get_text() for kw in ['é…å”®', 'é…ç§Ÿ', 'å®‰å±…æˆ¿', 'äººæ‰æˆ¿', 'å…¬ç§Ÿæˆ¿', 'ä¿éšœæˆ¿'])]

        # æå–ä¿¡æ¯
        for link in links[:20]:  # é™åˆ¶æœ€å¤šè·å–20æ¡
            try:
                title = link.get_text(strip=True)
                href = link.get('href', '')

                # è¿‡æ»¤ç›¸å…³å…¬å‘Š
                keywords = ['å®‰å±…æˆ¿', 'äººæ‰æˆ¿', 'å…¬ç§Ÿæˆ¿', 'ä¿éšœæˆ¿', 'é…å”®', 'é…ç§Ÿ', 'ä½æˆ¿']
                if not any(kw in title for kw in keywords):
                    continue

                # æ„å»ºå®Œæ•´URL
                if href.startswith('/'):
                    full_url = urljoin(base_url, href)
                elif not href.startswith('http'):
                    full_url = urljoin(base_url, '/' + href)
                else:
                    full_url = href

                # å°è¯•ä»æ ‡é¢˜æˆ–å‘¨å›´å…ƒç´ æå–æ—¥æœŸ
                date = self.extract_date(link, title)

                notices.append({
                    "title": title,
                    "url": full_url,
                    "date": date,
                    "source": self.get_source_name(base_url),
                    "fetched_at": datetime.now().isoformat()
                })
            except Exception as e:
                continue

        return notices

    def extract_date(self, link_element, title):
        """æå–æ—¥æœŸ"""
        import re

        # å°è¯•ä»æ ‡é¢˜ä¸­æå–æ—¥æœŸ
        date_match = re.search(r'(\d{4})[-å¹´](\d{1,2})[-æœˆ](\d{1,2})', title)
        if date_match:
            return f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"

        # å°è¯•ä»å‘¨å›´çš„spanã€timeç­‰å…ƒç´ è·å–
        parent = link_element.parent
        if parent:
            date_element = parent.find(['span', 'time', 'div'], class_=re.compile(r'date|time'))
            if date_element:
                date_text = date_element.get_text(strip=True)
                date_match = re.search(r'(\d{4})[-å¹´](\d{1,2})[-æœˆ](\d{1,2})', date_text)
                if date_match:
                    return f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"

        # é»˜è®¤è¿”å›ä»Šå¤©
        return datetime.now().strftime('%Y-%m-%d')

    def get_source_name(self, url):
        """æ ¹æ®URLè·å–æ¥æºåç§°"""
        for key, source in self.sources.items():
            if source['base_url'] in url:
                return source['name']
        return "æœªçŸ¥æ¥æº"

    def fetch_all_sources(self):
        """ä»æ‰€æœ‰æ•°æ®æºè·å–å…¬å‘Š"""
        all_notices = []
        cutoff_date = datetime.now() - timedelta(days=90)  # æœ€è¿‘90å¤©

        print("\n" + "=" * 80)
        print("å¼€å§‹æ”¶é›†ä¿éšœæˆ¿å…¬å‘Šä¿¡æ¯...")
        print("=" * 80)

        for source_key, source_info in self.sources.items():
            print(f"\nã€{source_info['name']}ã€‘")
            print(f"URL: {source_info['notice_url']}")

            html = self.fetch_page(source_info['notice_url'])
            if html:
                notices = self.parse_notice_list(html, source_info['base_url'])
                print(f"æ‰¾åˆ° {len(notices)} æ¡ç›¸å…³å…¬å‘Š")
                all_notices.extend(notices)

                # ç¤¼è²Œæ€§å»¶è¿Ÿ
                time.sleep(2)
            else:
                print(f"è·å–å¤±è´¥")

        # å»é‡å’Œè¿‡æ»¤
        unique_notices = self.deduplicate_notices(all_notices)
        recent_notices = [n for n in unique_notices
                         if datetime.strptime(n['date'], '%Y-%m-%d') >= cutoff_date]

        print("\n" + "=" * 80)
        print(f"æ€»è®¡æ‰¾åˆ° {len(unique_notices)} æ¡å”¯ä¸€å…¬å‘Šï¼ˆæœ€è¿‘90å¤©: {len(recent_notices)} æ¡ï¼‰")
        print("=" * 80)

        return recent_notices

    def deduplicate_notices(self, notices):
        """å»é‡ï¼ˆåŸºäºURLï¼‰"""
        seen_urls = set()
        unique = []

        for notice in notices:
            if notice['url'] not in seen_urls:
                seen_urls.add(notice['url'])
                unique.append(notice)

        return unique

    def save_notices(self, notices):
        """ä¿å­˜å…¬å‘Šåˆ°æ–‡ä»¶"""
        os.makedirs(self.config_dir, exist_ok=True)

        # è¯»å–å·²æœ‰æ•°æ®
        existing = []
        if os.path.exists(self.data_file):
            with open(self.data_file, 'r', encoding='utf-8') as f:
                existing = json.load(f)

        # åˆå¹¶æ–°æ•°æ®ï¼ˆåŸºäºURLå»é‡ï¼‰
        existing_urls = {n['url'] for n in existing}
        new_count = 0

        for notice in notices:
            if notice['url'] not in existing_urls:
                existing.insert(0, notice)  # æ–°æ•°æ®æ”¾åœ¨å‰é¢
                existing_urls.add(notice['url'])
                new_count += 1

        # ä¿å­˜
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file}")
        print(f"   æ–°å¢ {new_count} æ¡å…¬å‘Šï¼Œæ€»è®¡ {len(existing)} æ¡")

    def display_notices(self, notices, limit=15):
        """æ˜¾ç¤ºå…¬å‘Šåˆ—è¡¨"""
        if not notices:
            print("\næœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å…¬å‘Š")
            return

        print(f"\nğŸ“‹ æœ€æ–°ä¿éšœæˆ¿å…¬å‘Šï¼ˆæ˜¾ç¤ºæœ€è¿‘ {min(limit, len(notices))} æ¡ï¼‰ï¼š")
        print("=" * 80)

        for i, notice in enumerate(notices[:limit], 1):
            print(f"\n{i}. {notice['title'][:80]}...")
            print(f"   ğŸ“… å‘å¸ƒæ—¥æœŸ: {notice['date']}")
            print(f"   ğŸ¢ æ¥æº: {notice['source']}")
            print(f"   ğŸ”— é“¾æ¥: {notice['url']}")

        print("\n" + "=" * 80)

    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        print("\n" + "ğŸ " * 40)
        print("\næ·±åœ³å¸‚ä¿éšœæˆ¿æ•°æ®æ”¶é›†å™¨")
        print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # è·å–æ•°æ®
        notices = self.fetch_all_sources()

        # æ˜¾ç¤ºç»“æœ
        self.display_notices(notices)

        # ä¿å­˜æ•°æ®
        self.save_notices(notices)

        print("\nâœ… æ•°æ®æ”¶é›†å®Œæˆï¼")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("1. ä»¥ä¸Šä¿¡æ¯æ¥è‡ªå®˜æ–¹ç½‘ç«™ï¼Œè¯·ä»¥å®˜æ–¹å…¬å‘Šä¸ºå‡†")
        print("2. ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´çš„ç”³è¯·æ¡ä»¶å’Œæµç¨‹")
        print("3. æ³¨æ„ç”³è¯·æˆªæ­¢æ—¶é—´ï¼ŒåŠæ—¶æäº¤ææ–™")
        print("4. æ•°æ®å·²ä¿å­˜ï¼Œå¯éšæ—¶æŸ¥çœ‹å†å²è®°å½•")

def main():
    """ä¸»å‡½æ•°"""
    fetcher = HousingDataFetcher()
    fetcher.run()

if __name__ == "__main__":
    main()
