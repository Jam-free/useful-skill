#!/usr/bin/env python3
"""
çœŸå®çš„å…¬å‘ŠæŠ“å–è„šæœ¬ - ä»å®˜æ–¹æ¸ é“è·å–å‡†ç¡®ä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import re

# å®˜æ–¹ç½‘ç«™åˆ—è¡¨
official_sources = [
    {
        "name": "æ·±åœ³å¸‚ä½æˆ¿å’Œå»ºè®¾å±€",
        "url": "https://zjj.sz.gov.cn",
        "notice_page": "https://zjj.sz.gov.cn/xxgk/tzgg/index.shtml"
    },
    {
        "name": "ç¦ç”°åŒºä½å»ºå±€",
        "url": "https://www.szft.gov.cn",
        "notice_page": "https://www.szft.gov.cn/bmxx/qjsj/tzgg/index.shtml"
    },
    {
        "name": "é¾™ååŒºä½å»ºå±€",
        "url": "https://www.szlhq.gov.cn",
        "notice_page": "https://www.szlhq.gov.cn/lhq/zdfwgb/zfztgb/zxgg38/index.shtml"
    }
]

def fetch_page(url):
    """è·å–ç½‘é¡µå†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"è·å– {url} å¤±è´¥: {e}")
        return None

def parse_housing_notices(html, source_name):
    """è§£æä¿éšœæˆ¿å…¬å‘Š"""
    notices = []
    soup = BeautifulSoup(html, 'html.parser')

    # æŸ¥æ‰¾åŒ…å«å®‰å±…æˆ¿ã€äººæ‰æˆ¿ã€å…¬ç§Ÿæˆ¿ç­‰å…³é”®è¯çš„é“¾æ¥
    keywords = ['å®‰å±…æˆ¿', 'äººæ‰æˆ¿', 'å…¬ç§Ÿæˆ¿', 'ä¿éšœæˆ¿', 'é…å”®', 'é…ç§Ÿ', 'ä½æˆ¿']

    # å°è¯•ä¸åŒçš„é“¾æ¥é€‰æ‹©å™¨
    link_selectors = [
        'a[href*="/xxgk/tzgg/"]',
        'a[href*="content/post"]',
        '.notice-list a',
        '.article-list a',
        'ul li a'
    ]

    for selector in link_selectors:
        links = soup.select(selector)
        if links:
            break
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„åˆ—è¡¨ï¼Œè·å–æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)

    for link in links:
        try:
            title = link.get_text(strip=True)
            href = link.get('href', '')

            # è¿‡æ»¤ç›¸å…³å…¬å‘Š
            if any(keyword in title for keyword in keywords):
                # æ„å»ºå®Œæ•´URL
                if href.startswith('/'):
                    full_url = f"https://zjj.sz.gov.cn{href}"
                elif not href.startswith('http'):
                    full_url = f"https://zjj.sz.gov.cn/xxgk/tzgg/{href}"
                else:
                    full_url = href

                # å°è¯•æå–æ—¥æœŸ
                date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', title)
                if date_match:
                    date_str = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                else:
                    date_str = datetime.now().strftime('%Y-%m-%d')

                notices.append({
                    "title": title,
                    "url": full_url,
                    "date": date_str,
                    "source": source_name
                })
        except Exception as e:
            continue

    return notices

def get_recent_notices(days=30):
    """è·å–æœ€è¿‘Nå¤©çš„å…¬å‘Š"""
    print(f"\næ­£åœ¨æœç´¢æœ€è¿‘ {days} å¤©çš„ä¿éšœæˆ¿å…¬å‘Š...\n")
    all_notices = []
    cutoff_date = datetime.now() - timedelta(days=days)

    for source in official_sources:
        print(f"æœç´¢ {source['name']}...")

        # è·å–å…¬å‘Šåˆ—è¡¨é¡µ
        html = fetch_page(source['notice_page'])
        if html:
            notices = parse_housing_notices(html, source['name'])
            all_notices.extend(notices)
            print(f"  æ‰¾åˆ° {len(notices)} æ¡ç›¸å…³å…¬å‘Š")

    # æŒ‰æ—¥æœŸæ’åºå¹¶è¿‡æ»¤
    valid_notices = []
    for notice in all_notices:
        try:
            notice_date = datetime.strptime(notice['date'], '%Y-%m-%d')
            if notice_date >= cutoff_date:
                valid_notices.append(notice)
        except:
            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä¿ç•™è¿™æ¡è®°å½•
            valid_notices.append(notice)

    valid_notices.sort(key=lambda x: x['date'], reverse=True)
    return valid_notices

def display_notices(notices, limit=10):
    """æ˜¾ç¤ºå…¬å‘Šåˆ—è¡¨"""
    if not notices:
        print("\næœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å…¬å‘Š")
        return

    print(f"\næ‰¾åˆ° {len(notices)} æ¡å…¬å‘Šï¼ˆæ˜¾ç¤ºæœ€è¿‘ {min(limit, len(notices))} æ¡ï¼‰ï¼š\n")
    print("=" * 80)

    for i, notice in enumerate(notices[:limit], 1):
        print(f"\n{i}. {notice['title']}")
        print(f"   å‘å¸ƒæ—¥æœŸ: {notice['date']}")
        print(f"   æ¥æº: {notice['source']}")
        print(f"   é“¾æ¥: {notice['url']}")

    print("\n" + "=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("æ·±åœ³å¸‚ä¿éšœæˆ¿å…¬å‘ŠæŸ¥è¯¢ç³»ç»Ÿ")
    print("=" * 80)

    # è·å–æœ€è¿‘30å¤©çš„å…¬å‘Š
    notices = get_recent_notices(days=30)

    # æ˜¾ç¤ºç»“æœ
    display_notices(notices, limit=15)

    print("\nğŸ’¡ æç¤ºï¼š")
    print("1. ä»¥ä¸Šä¿¡æ¯æ¥è‡ªå®˜æ–¹ç½‘ç«™ï¼Œè¯·ä»¥å®˜æ–¹å…¬å‘Šä¸ºå‡†")
    print("2. ç‚¹å‡»é“¾æ¥æŸ¥çœ‹å®Œæ•´çš„ç”³è¯·æ¡ä»¶å’Œæµç¨‹")
    print("3. æ³¨æ„ç”³è¯·æˆªæ­¢æ—¶é—´ï¼ŒåŠæ—¶æäº¤ææ–™")

if __name__ == "__main__":
    main()
